# Apache Airflow Production Environment Configuration
# Copy this file to .env and fill in the values
# NEVER commit .env to version control!

# =============================================================================
# AIRFLOW CORE CONFIGURATION
# =============================================================================

# User ID for Airflow processes (use your system user ID: `id -u`)
AIRFLOW_UID=50000

# Airflow project directory (leave as . for current directory)
AIRFLOW_PROJ_DIR=.

# Environment file path
ENV_FILE_PATH=.env

# =============================================================================
# DATABASE CONFIGURATION (PostgreSQL)
# =============================================================================

# PostgreSQL credentials (CHANGE THESE IN PRODUCTION!)
POSTGRES_USER=airflow
POSTGRES_PASSWORD=CHANGE_ME_STRONG_PASSWORD_HERE
POSTGRES_DB=airflow

# Database pool configuration
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=10
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE=3600
AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=20

# =============================================================================
# REDIS CONFIGURATION (Celery Broker)
# =============================================================================

# Redis password (CHANGE THIS IN PRODUCTION!)
REDIS_PASSWORD=CHANGE_ME_REDIS_PASSWORD_HERE

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# Fernet key for encrypting secrets in database
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY=CHANGE_ME_GENERATE_FERNET_KEY_HERE

# Webserver secret key for Flask session encryption
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
AIRFLOW__WEBSERVER__SECRET_KEY=CHANGE_ME_GENERATE_SECRET_KEY_HERE

# Admin user credentials (CHANGE THESE IN PRODUCTION!)
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=CHANGE_ME_ADMIN_PASSWORD_HERE
_AIRFLOW_WWW_USER_EMAIL=admin@example.com
_AIRFLOW_WWW_USER_FIRSTNAME=Admin
_AIRFLOW_WWW_USER_LASTNAME=User
_AIRFLOW_WWW_USER_ROLE=Admin

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================

# Core Airflow parallelism settings
AIRFLOW__CORE__PARALLELISM=32
AIRFLOW__CORE__DAG_CONCURRENCY=16
AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=16
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=3

# Celery worker settings
AIRFLOW__CELERY__WORKER_CONCURRENCY=16
AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER=1

# Scheduler settings
AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=5
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=300
AIRFLOW__SCHEDULER__PARSING_PROCESSES=2
AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR=true

# Webserver settings
AIRFLOW__WEBSERVER__WORKERS=4
AIRFLOW__WEBSERVER__WORKER_CLASS=sync
AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL=30
AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE=1

# Number of Celery worker replicas (for scaling)
AIRFLOW_WORKER_REPLICAS=2

# =============================================================================
# WEBSERVER PORT CONFIGURATION
# =============================================================================

# External port for Airflow webserver
AIRFLOW_WEBSERVER_PORT=8080

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO

# Remote logging (set to true for cloud storage like S3, GCS, Azure Blob)
AIRFLOW__LOGGING__REMOTE_LOGGING=false
AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER=
AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID=

# =============================================================================
# METRICS AND MONITORING
# =============================================================================

# StatsD metrics (set to true to enable)
AIRFLOW__METRICS__STATSD_ON=false
AIRFLOW__METRICS__STATSD_HOST=statsd-exporter
AIRFLOW__METRICS__STATSD_PORT=9125
AIRFLOW__METRICS__STATSD_PREFIX=airflow

# =============================================================================
# EMAIL CONFIGURATION
# =============================================================================

# Email backend (for alerts and notifications)
AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.providers.sendgrid.utils.emailer.send_email
AIRFLOW__EMAIL__EMAIL_CONN_ID=sendgrid_default

# =============================================================================
# MONITORING SERVICES (Optional - used with --profile monitoring)
# =============================================================================

# Grafana admin credentials
GF_SECURITY_ADMIN_USER=admin
GF_SECURITY_ADMIN_PASSWORD=CHANGE_ME_GRAFANA_PASSWORD_HERE

# Grafana plugins to install (comma-separated)
GF_INSTALL_PLUGINS=

# =============================================================================
# ADDITIONAL SETTINGS
# =============================================================================

# Timezone (use IANA timezone names)
AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC

# DAG default timezone
AIRFLOW__CORE__DEFAULT_UI_TIMEZONE=UTC

# =============================================================================
# DEVELOPMENT/TESTING OVERRIDES (Comment out in production)
# =============================================================================

# Unit test mode (disable in production)
# AIRFLOW__CORE__UNIT_TEST_MODE=False

# Additional PIP requirements (for testing only - bake into Dockerfile for production)
# _PIP_ADDITIONAL_REQUIREMENTS=

# =============================================================================
# NOTES
# =============================================================================
# 
# 1. Generate Fernet Key:
#    python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
#
# 2. Generate Secret Key:
#    python -c "import secrets; print(secrets.token_urlsafe(32))"
#
# 3. Get your user ID:
#    id -u
#
# 4. For production deployments:
#    - Use a secrets management service (AWS Secrets Manager, HashiCorp Vault, etc.)
#    - Rotate credentials regularly
#    - Use strong, unique passwords
#    - Enable SSL/TLS for all connections
#    - Configure firewall rules
#    - Enable audit logging
#    - Set up monitoring and alerting
#
