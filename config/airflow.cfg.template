# Apache Airflow Production Configuration Template
# 
# This file provides production-ready configuration options.
# Most settings can be overridden via environment variables (AIRFLOW__SECTION__KEY)
# 
# Documentation: https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html

[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
logging_level = INFO

# The executor class that airflow should use
executor = CeleryExecutor

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently per DAG
dag_concurrency = 16

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 3

# The maximum number of active tasks per DAG
max_active_tasks_per_dag = 16

# Whether to load the DAG examples that ship with Airflow
load_examples = False

# Whether to load the default connections that ship with Airflow
load_default_connections = False

# Path to the folder containing Airflow plugins
plugins_folder = /opt/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = ${AIRFLOW__CORE__FERNET_KEY}

# Whether to disable pickling dags
donot_pickle = True

# How long before timing out a python file import
dagbag_import_timeout = 30

# The DAG file's folder list
dags_folder = /opt/airflow/dags

# Default timezone
default_timezone = UTC

# Whether new dags are paused by default at creation
dags_are_paused_at_creation = True

# Maximum number of Rendered Task Instance Fields (Template Fields) per task
max_num_rendered_ti_fields_per_task = 30

# Whether to enable pickling for xcom (note that this is insecure)
enable_xcom_pickling = False

# Kill tasks on SchedulerJob heartbeat failure (production: True)
killed_task_cleanup_time = 86400

[database]
# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}

# The pool size for SqlAlchemy
sql_alchemy_pool_size = 10

# The maximum overflow size of the pool
sql_alchemy_max_overflow = 20

# The SqlAlchemy pool recycle time (in seconds)
sql_alchemy_pool_recycle = 3600

# If this is True, sqlalchemy will log all SQL queries
sql_alchemy_echo = False

# Number of times to retry a task instance before failing it
sql_alchemy_connect_args = 

[scheduler]
# Task instances listen for external kill signal
run_duration = -1

# The scheduler will run this number of processes in parallel
processor_poll_interval = 1

# The minimum number of seconds between file parsing
min_file_process_interval = 30

# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 300

# How long to wait before checking for new files in DAGs directory
min_file_parsing_loop_time = 1

# The number of seconds to wait before timing out on DAG file processing
dag_file_processor_timeout = 50

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# The scheduler will use this time to calculate run_after
max_tis_per_query = 512

# Statsd (daemon from Etsy) is used to send metrics to in statsd compatible format
statsd_on = False
statsd_host = statsd-exporter
statsd_port = 9125
statsd_prefix = airflow

# Number of processes for the scheduler to use for DAG file parsing
parsing_processes = 2

# How often to check for orphaned tasks
orphaned_tasks_check_interval = 300.0

# How often to check for zombie tasks
zombie_detection_interval = 10.0

# Enable the scheduler health check
enable_health_check = True

# Use job pool for job scheduling
use_job_schedule = True

[webserver]
# The base url of your website
base_url = http://localhost:8080

# Number of workers to run the webserver on
workers = 4

# Worker timeout from Gunicorn
worker_timeout = 120

# Worker class to use for Gunicorn
worker_class = sync

# Expose the configuration file in the web UI
expose_config = False

# Expose hostname in the UI
expose_hostname = False

# Expose stacktrace in the UI
expose_stacktrace = False

# The port on which to run the web server
web_server_port = 8080

# The host on which to run the web server
web_server_host = 0.0.0.0

# Secret key for Flask session
secret_key = ${AIRFLOW__WEBSERVER__SECRET_KEY}

# Number of seconds the webserver will wait before timing out on a page load
web_server_master_timeout = 120

# Number of workers to refresh at a time
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers
worker_refresh_interval = 30

# Whether to enable authentication
authenticate = True

# Filter the list of dags by owner name
filter_by_owner = False

# Default DAG view
default_ui_view = tree

# Default DAG orientation
default_dag_view = tree

# Default wrap mode for DAG code
default_wrap = False

# Amount of time (in secs) webserver will wait for initial handshake
web_server_ssl_cert = 
web_server_ssl_key = 

# Whether to enable Rate limiting
rbac_user_registration_role = Public

# OAuth configuration
oauth_providers = 

# Enable/disable rate limiting
enable_rate_limit = True

[celery]
# The app name that will be used by celery
celery_app_name = airflow.providers.celery.executors.celery_executor

# The concurrency that will be used when starting workers with the airflow celery worker
worker_concurrency = 16

# When not using a pool, the number of prefetch multiplier for the worker
worker_prefetch_multiplier = 1

# The Celery broker URL
broker_url = ${AIRFLOW__CELERY__BROKER_URL}

# The Celery result backend
result_backend = ${AIRFLOW__CELERY__RESULT_BACKEND}

# Celery Flower is a sweet UI for Celery
flower_host = 0.0.0.0
flower_port = 5555

# Default queue
default_queue = default

# How often to sync up celery with the database (in seconds)
sync_parallelism = 0

# Import path for celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

# Worker log server port
worker_log_server_port = 8793

# Umask for celery worker tasks
worker_umask = 0o077

# Task adoption
task_adoption_timeout = 600

# Stalled task timeout
stalled_task_timeout = 0

# Task publish max retries
task_publish_max_retries = 3

# Worker enable remote control
worker_enable_remote_control = True

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Logging level
logging_level = INFO

# Logging class
logging_config_class = 

# Log format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename template
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log

# Log processor filename template
log_processor_filename_template = {{ filename }}.log

# Remote logging
remote_logging = False
remote_base_log_folder = 
remote_log_conn_id = 

# Use server-side encryption for S3 logs
encrypt_s3_logs = False

# Logging level for celery
celery_logging_level = INFO

# Colored console logging
colored_console_log = True

# Colored log format
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

[api]
# Authentication backend
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

# Enable the experimental API
enable_experimental_api = False

# Maximum number of items returned per request
maximum_page_limit = 100

# Fallback page limit
fallback_page_limit = 100

[operators]
# The default owner assigned to each new operator
default_owner = airflow

# Default queue
default_queue = default

# Default number of retries for operators
default_retries = 1

# Default retry delay in seconds
default_retry_delay = 300

[email]
# Email backend
email_backend = airflow.providers.sendgrid.utils.emailer.send_email

# SMTP configuration (if using SMTP backend)
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@example.com
smtp_user = 
smtp_password = 

[metrics]
# Enable sending metrics to StatsD
statsd_on = False
statsd_host = statsd-exporter
statsd_port = 9125
statsd_prefix = airflow

# Allow sending metrics
statsd_allow_list = 

[secrets]
# Backend for secrets (can be configured for Vault, AWS Secrets Manager, etc.)
backend = 
backend_kwargs = 
